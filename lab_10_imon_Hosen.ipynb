{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Neural Networks and Data Science***  \n",
    "Lab #10    \n",
    "Date: 08.01.2025  \n",
    "Deadline: 15.01.2025, 16:00  \n",
    "  \n",
    "Submitted By: ***Imon Hosen***  \n",
    " \n",
    "  \n",
    "Submitted To: ***Dr. Marcel Völschow***  \n",
    "\n",
    "---\n",
    "\n",
    "## Lab Session 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 1 \n",
    "\n",
    "Machine learning scientists and engineers came up with a number of auxiliary layers that can improve the\n",
    "training process, less over-fitting and better generalization. Two popular examples are:\n",
    "\n",
    "- layers.Dropout(frac) randomly sets a fraction frac of input nodes to 0 and rescales the other inputs\n",
    "to retain normalization, which reduces a network’s tendency to overfit to certain features of the input\n",
    "- layers.Dropout(frac) randomly sets a fraction frac of input nodes to 0 and rescales the other inputs\n",
    "to retain normalization, which reduces a network’s tendency to overfit to certain features of the input\n",
    "\n",
    "On the CIFAR-10, a subset of the (huge) ImageNET dataset of classified color images. Typical perceptron-style\n",
    "models achieve test data accuracies below 50 %.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Use\n",
    "```python\n",
    "(train_img ,train_lab) ,(test_img ,test_lab) = keras.datasets.cifar10.load_data ()\n",
    "```\n",
    "to load the dataset. Make sure the pixels are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_img, train_lab), (test_img, test_lab) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_img = train_img.astype('float32') / 255.0\n",
    "test_img = test_img.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Set up a basic CNN with a single convolutional layer, 64 filters, a (3,3) kernel, \"same\" padding and\n",
    "ReLU activation, followed by a (2,2) max pooling and a dense layer with 128 nodes (ReLU), concluded\n",
    "by a softmaxed output layer. Train it for up to 30 epochs. What’s the test data accuracy before it\n",
    "starts to overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nnds-7b/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1736694967.090283   53060 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736694968.549770   53686 service.cc:148] XLA service 0x762ebc008160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736694968.549901   53686 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2025-01-12 16:16:08.565119: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1736694968.619279   53686 cuda_dnn.cc:529] Loaded cuDNN version 90600\n",
      "I0000 00:00:1736694969.429542   53686 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 3s - 4ms/step - accuracy: 0.4669 - loss: 1.5030 - val_accuracy: 0.5620 - val_loss: 1.2475\n",
      "Epoch 2/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.5904 - loss: 1.1679 - val_accuracy: 0.5856 - val_loss: 1.1753\n",
      "Epoch 3/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.6371 - loss: 1.0425 - val_accuracy: 0.6002 - val_loss: 1.1356\n",
      "Epoch 4/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.6657 - loss: 0.9579 - val_accuracy: 0.6393 - val_loss: 1.0409\n",
      "Epoch 5/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.6946 - loss: 0.8790 - val_accuracy: 0.6461 - val_loss: 1.0192\n",
      "Epoch 6/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.7189 - loss: 0.8114 - val_accuracy: 0.6396 - val_loss: 1.0500\n",
      "Epoch 7/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.7396 - loss: 0.7486 - val_accuracy: 0.6459 - val_loss: 1.0381\n",
      "Epoch 8/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.7592 - loss: 0.6878 - val_accuracy: 0.6536 - val_loss: 1.0496\n",
      "Epoch 9/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.7786 - loss: 0.6361 - val_accuracy: 0.6544 - val_loss: 1.0459\n",
      "Epoch 10/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.7957 - loss: 0.5817 - val_accuracy: 0.6588 - val_loss: 1.0712\n",
      "Epoch 11/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8139 - loss: 0.5348 - val_accuracy: 0.6486 - val_loss: 1.1221\n",
      "Epoch 12/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8281 - loss: 0.4905 - val_accuracy: 0.6549 - val_loss: 1.1219\n",
      "Epoch 13/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8480 - loss: 0.4395 - val_accuracy: 0.6527 - val_loss: 1.1889\n",
      "Epoch 14/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8613 - loss: 0.4004 - val_accuracy: 0.6535 - val_loss: 1.2385\n",
      "Epoch 15/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8765 - loss: 0.3615 - val_accuracy: 0.6439 - val_loss: 1.3542\n",
      "Epoch 16/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8882 - loss: 0.3245 - val_accuracy: 0.6463 - val_loss: 1.3418\n",
      "Epoch 17/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.8994 - loss: 0.2921 - val_accuracy: 0.6426 - val_loss: 1.4741\n",
      "Epoch 18/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9135 - loss: 0.2569 - val_accuracy: 0.6484 - val_loss: 1.4883\n",
      "Epoch 19/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9205 - loss: 0.2343 - val_accuracy: 0.6470 - val_loss: 1.5995\n",
      "Epoch 20/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9302 - loss: 0.2098 - val_accuracy: 0.6396 - val_loss: 1.6697\n",
      "Epoch 21/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9358 - loss: 0.1914 - val_accuracy: 0.6412 - val_loss: 1.7411\n",
      "Epoch 22/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9440 - loss: 0.1676 - val_accuracy: 0.6409 - val_loss: 1.8056\n",
      "Epoch 23/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9467 - loss: 0.1576 - val_accuracy: 0.6437 - val_loss: 1.8424\n",
      "Epoch 24/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9526 - loss: 0.1447 - val_accuracy: 0.6404 - val_loss: 1.9337\n",
      "Epoch 25/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9588 - loss: 0.1252 - val_accuracy: 0.6426 - val_loss: 2.0018\n",
      "Epoch 26/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9598 - loss: 0.1217 - val_accuracy: 0.6401 - val_loss: 2.0641\n",
      "Epoch 27/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9651 - loss: 0.1077 - val_accuracy: 0.6405 - val_loss: 2.1579\n",
      "Epoch 28/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9671 - loss: 0.1012 - val_accuracy: 0.6338 - val_loss: 2.3097\n",
      "Epoch 29/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9641 - loss: 0.1092 - val_accuracy: 0.6329 - val_loss: 2.3215\n",
      "Epoch 30/30\n",
      "782/782 - 1s - 1ms/step - accuracy: 0.9697 - loss: 0.0929 - val_accuracy: 0.6378 - val_loss: 2.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 16:16:40.206765: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - 1ms/step - accuracy: 0.6378 - loss: 2.4271\n",
      "Final Test Accuracy: 63.78%\n"
     ]
    }
   ],
   "source": [
    "train_lab = keras.utils.to_categorical(train_lab, 10)\n",
    "test_lab = keras.utils.to_categorical(test_lab, 10)\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_img, train_lab, epochs=30, batch_size=64,\n",
    "                    validation_data=(test_img, test_lab),\n",
    "                    verbose=2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_img, test_lab, verbose=2)\n",
    "print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy peaks at **Epoch 10 (65.88%)** and declines afterward, while the training accuracy continues to improve, reaching **96.97%** by Epoch **30**. This divergence indicates that the model begins overfitting to the training data after **Epoch 10**, losing its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Set up a sophisticated CNN with ker=(3,3) and the following layers:\n",
    "```python\n",
    "Input(shape=(32,32,3))\n",
    "Conv2D(filters=32, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization()\n",
    "Conv2D(filters=32, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization())\n",
    "MaxPool2D(pool_size=(2, 2))\n",
    "Dropout(0.25)\n",
    "Conv2D(filters=64, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization()\n",
    "Conv2D(filters=64, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization()\n",
    "MaxPool2D(pool_size=(2, 2))\n",
    "Dropout(0.25)\n",
    "Conv2D(filters=128, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization()\n",
    "Conv2D(filters=128, kernel_size=ker, activation=\"relu\", padding=\"same\")\n",
    "BatchNormalization()\n",
    "MaxPool2D(pool_size=(2, 2))\n",
    "Dropout(0.25)\n",
    "Flatten()\n",
    "Dense(256, activation=\"relu\")\n",
    "Dropout(0.25)\n",
    "Dense(10, activation=\"softmax\")\n",
    "```\n",
    "Train the model for up to 100 epochs. How well does it perform on the test data before it starts to\n",
    "overfit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = (3, 3)\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "\n",
    "    layers.Conv2D(filters=32, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=128, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=128, kernel_size=ker, activation=\"relu\", padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 - 10s - 12ms/step - accuracy: 0.4701 - loss: 1.4922 - val_accuracy: 0.5800 - val_loss: 1.1899\n",
      "Epoch 2/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.6353 - loss: 1.0261 - val_accuracy: 0.6316 - val_loss: 1.0488\n",
      "Epoch 3/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.7019 - loss: 0.8535 - val_accuracy: 0.7316 - val_loss: 0.7670\n",
      "Epoch 4/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.7404 - loss: 0.7502 - val_accuracy: 0.7413 - val_loss: 0.7469\n",
      "Epoch 5/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.7589 - loss: 0.6836 - val_accuracy: 0.7791 - val_loss: 0.6347\n",
      "Epoch 6/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.7828 - loss: 0.6228 - val_accuracy: 0.7789 - val_loss: 0.6594\n",
      "Epoch 7/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8035 - loss: 0.5666 - val_accuracy: 0.7834 - val_loss: 0.6428\n",
      "Epoch 8/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8183 - loss: 0.5281 - val_accuracy: 0.7828 - val_loss: 0.6580\n",
      "Epoch 9/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8303 - loss: 0.4925 - val_accuracy: 0.7949 - val_loss: 0.6195\n",
      "Epoch 10/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8393 - loss: 0.4571 - val_accuracy: 0.8230 - val_loss: 0.5284\n",
      "Epoch 11/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8547 - loss: 0.4190 - val_accuracy: 0.8210 - val_loss: 0.5462\n",
      "Epoch 12/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8575 - loss: 0.4024 - val_accuracy: 0.8227 - val_loss: 0.5452\n",
      "Epoch 13/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8690 - loss: 0.3772 - val_accuracy: 0.8130 - val_loss: 0.6013\n",
      "Epoch 14/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8751 - loss: 0.3561 - val_accuracy: 0.8270 - val_loss: 0.5545\n",
      "Epoch 15/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8806 - loss: 0.3390 - val_accuracy: 0.7917 - val_loss: 0.6641\n",
      "Epoch 16/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8876 - loss: 0.3204 - val_accuracy: 0.8247 - val_loss: 0.5321\n",
      "Epoch 17/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8928 - loss: 0.3056 - val_accuracy: 0.8309 - val_loss: 0.5618\n",
      "Epoch 18/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.8978 - loss: 0.2898 - val_accuracy: 0.8409 - val_loss: 0.5182\n",
      "Epoch 19/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9018 - loss: 0.2793 - val_accuracy: 0.8099 - val_loss: 0.6525\n",
      "Epoch 20/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9073 - loss: 0.2629 - val_accuracy: 0.8463 - val_loss: 0.5177\n",
      "Epoch 21/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9129 - loss: 0.2492 - val_accuracy: 0.8416 - val_loss: 0.5526\n",
      "Epoch 22/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9156 - loss: 0.2447 - val_accuracy: 0.8487 - val_loss: 0.5137\n",
      "Epoch 23/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9184 - loss: 0.2351 - val_accuracy: 0.8450 - val_loss: 0.5408\n",
      "Epoch 24/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9209 - loss: 0.2238 - val_accuracy: 0.8470 - val_loss: 0.5429\n",
      "Epoch 25/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9243 - loss: 0.2205 - val_accuracy: 0.7895 - val_loss: 0.7857\n",
      "Epoch 26/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9289 - loss: 0.2070 - val_accuracy: 0.8507 - val_loss: 0.5450\n",
      "Epoch 27/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9290 - loss: 0.2031 - val_accuracy: 0.8453 - val_loss: 0.5414\n",
      "Epoch 28/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9313 - loss: 0.1987 - val_accuracy: 0.8411 - val_loss: 0.5495\n",
      "Epoch 29/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9343 - loss: 0.1908 - val_accuracy: 0.8222 - val_loss: 0.6441\n",
      "Epoch 30/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9357 - loss: 0.1857 - val_accuracy: 0.8541 - val_loss: 0.5351\n",
      "Epoch 31/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9354 - loss: 0.1841 - val_accuracy: 0.8579 - val_loss: 0.5197\n",
      "Epoch 32/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9387 - loss: 0.1754 - val_accuracy: 0.8482 - val_loss: 0.5486\n",
      "Epoch 33/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9414 - loss: 0.1695 - val_accuracy: 0.8455 - val_loss: 0.5690\n",
      "Epoch 34/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9403 - loss: 0.1696 - val_accuracy: 0.8471 - val_loss: 0.5526\n",
      "Epoch 35/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9437 - loss: 0.1657 - val_accuracy: 0.8442 - val_loss: 0.6307\n",
      "Epoch 36/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9453 - loss: 0.1553 - val_accuracy: 0.8500 - val_loss: 0.5789\n",
      "Epoch 37/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9494 - loss: 0.1509 - val_accuracy: 0.8543 - val_loss: 0.5360\n",
      "Epoch 38/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9465 - loss: 0.1530 - val_accuracy: 0.8529 - val_loss: 0.5498\n",
      "Epoch 39/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9489 - loss: 0.1500 - val_accuracy: 0.8517 - val_loss: 0.5642\n",
      "Epoch 40/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9498 - loss: 0.1459 - val_accuracy: 0.8482 - val_loss: 0.5887\n",
      "Epoch 41/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9505 - loss: 0.1446 - val_accuracy: 0.8534 - val_loss: 0.5534\n",
      "Epoch 42/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9520 - loss: 0.1408 - val_accuracy: 0.8522 - val_loss: 0.5548\n",
      "Epoch 43/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9522 - loss: 0.1350 - val_accuracy: 0.8530 - val_loss: 0.5814\n",
      "Epoch 44/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9529 - loss: 0.1376 - val_accuracy: 0.8592 - val_loss: 0.5668\n",
      "Epoch 45/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9558 - loss: 0.1294 - val_accuracy: 0.8568 - val_loss: 0.5700\n",
      "Epoch 46/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9555 - loss: 0.1280 - val_accuracy: 0.8561 - val_loss: 0.5776\n",
      "Epoch 47/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9555 - loss: 0.1316 - val_accuracy: 0.8533 - val_loss: 0.5732\n",
      "Epoch 48/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9560 - loss: 0.1285 - val_accuracy: 0.8543 - val_loss: 0.5862\n",
      "Epoch 49/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9581 - loss: 0.1223 - val_accuracy: 0.8558 - val_loss: 0.5673\n",
      "Epoch 50/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9583 - loss: 0.1202 - val_accuracy: 0.8547 - val_loss: 0.5691\n",
      "Epoch 51/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9604 - loss: 0.1170 - val_accuracy: 0.8543 - val_loss: 0.5711\n",
      "Epoch 52/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9599 - loss: 0.1176 - val_accuracy: 0.8550 - val_loss: 0.5983\n",
      "Epoch 53/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9599 - loss: 0.1174 - val_accuracy: 0.8505 - val_loss: 0.6148\n",
      "Epoch 54/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9606 - loss: 0.1148 - val_accuracy: 0.8614 - val_loss: 0.5940\n",
      "Epoch 55/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9617 - loss: 0.1123 - val_accuracy: 0.8587 - val_loss: 0.6065\n",
      "Epoch 56/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9633 - loss: 0.1063 - val_accuracy: 0.8595 - val_loss: 0.5856\n",
      "Epoch 57/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9622 - loss: 0.1115 - val_accuracy: 0.8591 - val_loss: 0.5701\n",
      "Epoch 58/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9622 - loss: 0.1124 - val_accuracy: 0.8599 - val_loss: 0.5657\n",
      "Epoch 59/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9647 - loss: 0.1046 - val_accuracy: 0.8595 - val_loss: 0.5909\n",
      "Epoch 60/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9643 - loss: 0.1064 - val_accuracy: 0.8553 - val_loss: 0.6121\n",
      "Epoch 61/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9637 - loss: 0.1047 - val_accuracy: 0.8566 - val_loss: 0.6145\n",
      "Epoch 62/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9650 - loss: 0.1022 - val_accuracy: 0.8609 - val_loss: 0.6124\n",
      "Epoch 63/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9651 - loss: 0.1002 - val_accuracy: 0.8627 - val_loss: 0.5996\n",
      "Epoch 64/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9672 - loss: 0.0940 - val_accuracy: 0.8591 - val_loss: 0.6345\n",
      "Epoch 65/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9653 - loss: 0.1015 - val_accuracy: 0.8595 - val_loss: 0.6034\n",
      "Epoch 66/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9657 - loss: 0.1017 - val_accuracy: 0.8585 - val_loss: 0.5665\n",
      "Epoch 67/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9676 - loss: 0.0939 - val_accuracy: 0.8584 - val_loss: 0.6222\n",
      "Epoch 68/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9693 - loss: 0.0915 - val_accuracy: 0.8615 - val_loss: 0.6055\n",
      "Epoch 69/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9671 - loss: 0.0960 - val_accuracy: 0.8561 - val_loss: 0.6065\n",
      "Epoch 70/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9689 - loss: 0.0932 - val_accuracy: 0.8503 - val_loss: 0.6649\n",
      "Epoch 71/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9686 - loss: 0.0906 - val_accuracy: 0.8624 - val_loss: 0.5872\n",
      "Epoch 72/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9679 - loss: 0.0915 - val_accuracy: 0.8565 - val_loss: 0.5971\n",
      "Epoch 73/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9690 - loss: 0.0913 - val_accuracy: 0.8560 - val_loss: 0.6378\n",
      "Epoch 74/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9698 - loss: 0.0921 - val_accuracy: 0.8563 - val_loss: 0.6088\n",
      "Epoch 75/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9700 - loss: 0.0906 - val_accuracy: 0.8560 - val_loss: 0.6007\n",
      "Epoch 76/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9705 - loss: 0.0891 - val_accuracy: 0.8597 - val_loss: 0.6284\n",
      "Epoch 77/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9709 - loss: 0.0867 - val_accuracy: 0.8617 - val_loss: 0.6280\n",
      "Epoch 78/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9710 - loss: 0.0845 - val_accuracy: 0.8601 - val_loss: 0.6336\n",
      "Epoch 79/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9701 - loss: 0.0863 - val_accuracy: 0.8623 - val_loss: 0.6354\n",
      "Epoch 80/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9720 - loss: 0.0830 - val_accuracy: 0.8571 - val_loss: 0.6641\n",
      "Epoch 81/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9726 - loss: 0.0806 - val_accuracy: 0.8600 - val_loss: 0.6303\n",
      "Epoch 82/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9714 - loss: 0.0838 - val_accuracy: 0.8629 - val_loss: 0.6025\n",
      "Epoch 83/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9726 - loss: 0.0821 - val_accuracy: 0.8606 - val_loss: 0.6361\n",
      "Epoch 84/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9735 - loss: 0.0799 - val_accuracy: 0.8614 - val_loss: 0.6171\n",
      "Epoch 85/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9719 - loss: 0.0824 - val_accuracy: 0.8602 - val_loss: 0.6031\n",
      "Epoch 86/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9727 - loss: 0.0791 - val_accuracy: 0.8607 - val_loss: 0.6429\n",
      "Epoch 87/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9730 - loss: 0.0791 - val_accuracy: 0.8669 - val_loss: 0.5995\n",
      "Epoch 88/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9735 - loss: 0.0773 - val_accuracy: 0.8639 - val_loss: 0.6222\n",
      "Epoch 89/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9727 - loss: 0.0787 - val_accuracy: 0.8597 - val_loss: 0.6286\n",
      "Epoch 90/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9741 - loss: 0.0767 - val_accuracy: 0.8683 - val_loss: 0.5913\n",
      "Epoch 91/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9744 - loss: 0.0762 - val_accuracy: 0.8664 - val_loss: 0.6124\n",
      "Epoch 92/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9738 - loss: 0.0764 - val_accuracy: 0.8672 - val_loss: 0.5995\n",
      "Epoch 93/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9745 - loss: 0.0762 - val_accuracy: 0.8660 - val_loss: 0.6161\n",
      "Epoch 94/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9752 - loss: 0.0741 - val_accuracy: 0.8671 - val_loss: 0.5716\n",
      "Epoch 95/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9749 - loss: 0.0755 - val_accuracy: 0.8653 - val_loss: 0.6076\n",
      "Epoch 96/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9757 - loss: 0.0731 - val_accuracy: 0.8551 - val_loss: 0.6231\n",
      "Epoch 97/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9741 - loss: 0.0748 - val_accuracy: 0.8564 - val_loss: 0.6224\n",
      "Epoch 98/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9765 - loss: 0.0711 - val_accuracy: 0.8671 - val_loss: 0.5974\n",
      "Epoch 99/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9744 - loss: 0.0737 - val_accuracy: 0.8564 - val_loss: 0.6282\n",
      "Epoch 100/100\n",
      "782/782 - 2s - 3ms/step - accuracy: 0.9744 - loss: 0.0774 - val_accuracy: 0.8639 - val_loss: 0.6187\n",
      "313/313 - 1s - 2ms/step - accuracy: 0.8639 - loss: 0.6187\n",
      "Test Accuracy: 86.39%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_img, train_lab,\n",
    "                    validation_data=(test_img, test_lab),\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_img, test_lab, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After epoch 50, the validation accuracy fluctuates and eventually drops, which indicates that the model is memorizing the training data rather than learning general patterns. This suggests that continuing training beyond this point could lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 2\n",
    "Neural networks are used in medicine to assist doctors in their everyday work. One application is the\n",
    "detection of bacterial pneunomia in chest x-ray scans. /share/xray contains a total of 5860 scans divided into\n",
    "a training, validation and test data set, each containing a NORMAL folder with healthy lungs and PNEUNOMIA\n",
    "folder with infected lungs.   \n",
    "To get the data into your notebook, you can use something like\n",
    "\n",
    "```python\n",
    "from os import listdir\n",
    "from os.path import isfile , join\n",
    "path = \"train/NORMAL/\"\n",
    "scan_names = [f for f in listdir(path) if isfile(join(path ,f))]\n",
    "```\n",
    "\n",
    "Because the data contains only a single channel, 2D convolutional layers will require an additional (dummy)\n",
    "dimension added via np.expand_dims. Experiment with different networks starting with the CIFAR-10 net.\n",
    "Alternatively, you can replace 2D convolutions with tf.keras.layers.Conv1D as shown in Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal images: 1343\n",
      "Number of pneumonia images: 3877\n"
     ]
    }
   ],
   "source": [
    "zip_file_path = '/share/xray/xray.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    file_list = zip_ref.namelist()\n",
    "    \n",
    "    # Filter out images in the 'train' folder\n",
    "    train_files = [file for file in file_list if 'train' in file]\n",
    "    \n",
    "    # Filter out the NORMAL and PNEUMONIA folders\n",
    "    normal_train_files = [file for file in train_files if 'train/NORMAL/' in file]\n",
    "    pneumonia_train_files = [file for file in train_files if 'train/PNEUMONIA/' in file]\n",
    "\n",
    "print(f\"Number of normal images: {len(normal_train_files)}\")\n",
    "print(f\"Number of pneumonia images: {len(pneumonia_train_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading train/NORMAL/: cannot identify image file <_io.BytesIO object at 0x76303b598040>\n",
      "Error loading train/NORMAL/.DS_Store: cannot identify image file <_io.BytesIO object at 0x76303b57f420>\n",
      "Error loading train/PNEUMONIA/: cannot identify image file <_io.BytesIO object at 0x7630442e13f0>\n",
      "Error loading train/PNEUMONIA/.DS_Store: cannot identify image file <_io.BytesIO object at 0x76303b5ed5d0>\n",
      "Loaded 1341 normal images\n",
      "Loaded 3875 pneumonia images\n"
     ]
    }
   ],
   "source": [
    "def load_image(zip_ref, file):\n",
    "    try:\n",
    "        with zip_ref.open(file) as img_file:\n",
    "            img_data = img_file.read()\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            img = img.resize((150, 150))\n",
    "            img = img.convert('L')  \n",
    "            img_array = img_to_array(img)  \n",
    "            img_array = np.expand_dims(img_array, axis=-1) \n",
    "            return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "normal_images = []\n",
    "for file in normal_train_files:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        img = load_image(zip_ref, file)\n",
    "        if img is not None:\n",
    "            normal_images.append(img)\n",
    "\n",
    "pneumonia_images = []\n",
    "for file in pneumonia_train_files:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        img = load_image(zip_ref, file)\n",
    "        if img is not None:\n",
    "            pneumonia_images.append(img)\n",
    "\n",
    "print(f\"Loaded {len(normal_images)} normal images\")\n",
    "print(f\"Loaded {len(pneumonia_images)} pneumonia images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (4172, 150, 150, 1, 1)\n",
      "Validation data shape: (1044, 150, 150, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "normal_labels = [0] * len(normal_images)\n",
    "pneumonia_labels = [1] * len(pneumonia_images)\n",
    "\n",
    "all_images = normal_images + pneumonia_images\n",
    "all_labels = normal_labels + pneumonia_labels\n",
    "\n",
    "all_images = np.array(all_images) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_labels = to_categorical(all_labels, num_classes=2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nnds-7b/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7339 - loss: 0.6101 - val_accuracy: 0.9368 - val_loss: 0.2851\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9200 - loss: 0.2564 - val_accuracy: 0.9272 - val_loss: 0.2723\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9469 - loss: 0.2075 - val_accuracy: 0.9511 - val_loss: 0.2120\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9549 - loss: 0.1963 - val_accuracy: 0.9406 - val_loss: 0.2069\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9631 - loss: 0.1650 - val_accuracy: 0.9598 - val_loss: 0.1837\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9657 - loss: 0.1446 - val_accuracy: 0.9425 - val_loss: 0.1849\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9634 - loss: 0.1428 - val_accuracy: 0.9607 - val_loss: 0.1484\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9731 - loss: 0.1183 - val_accuracy: 0.9674 - val_loss: 0.1567\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9743 - loss: 0.1098 - val_accuracy: 0.9665 - val_loss: 0.1402\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9754 - loss: 0.0986 - val_accuracy: 0.9665 - val_loss: 0.1215\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1187\n",
      "Test accuracy: 0.9664750695228577\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  \n",
    "    layers.MaxPooling2D((2, 2)),  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  \n",
    "    layers.Flatten(), \n",
    "    layers.Dense(64, activation='relu'), \n",
    "    layers.Dense(2, activation='softmax') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CIFAR10 Network***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.8579 - loss: 4.2664 - val_accuracy: 0.8180 - val_loss: 1.5375\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9693 - loss: 0.0871 - val_accuracy: 0.7251 - val_loss: 3.6092\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9697 - loss: 0.0819 - val_accuracy: 0.7672 - val_loss: 0.7720\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9812 - loss: 0.0523 - val_accuracy: 0.9090 - val_loss: 0.2941\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9811 - loss: 0.0451 - val_accuracy: 0.8927 - val_loss: 0.3602\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9888 - loss: 0.0337 - val_accuracy: 0.8745 - val_loss: 1.8228\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9896 - loss: 0.0258 - val_accuracy: 0.9531 - val_loss: 0.1070\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9922 - loss: 0.0237 - val_accuracy: 0.9751 - val_loss: 0.0766\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9918 - loss: 0.0172 - val_accuracy: 0.9646 - val_loss: 0.1430\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9929 - loss: 0.0187 - val_accuracy: 0.8812 - val_loss: 0.3931\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8789 - loss: 0.4284\n",
      "Test accuracy: 0.8812260627746582\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),  # First convolution layer\n",
    "    layers.BatchNormalization(),  # Normalize activations\n",
    "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  # Second convolution layer\n",
    "    layers.BatchNormalization(),  # Normalize activations\n",
    "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),  # Third convolution layer\n",
    "    layers.BatchNormalization(),  # Normalize activations\n",
    "    layers.MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    layers.Flatten(),  # Flatten the output\n",
    "    layers.Dense(256, activation='relu'),  # Fully connected layer\n",
    "    layers.Dense(2, activation='softmax')  # Output layer with softmax for classification (normal vs pneumonia)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 3\n",
    "Stars can be characterized by the spectrum of light they emit. In simple terms, the spectrum of a star\n",
    "quantifies how brightly it shines within a given wavelength bin. At closer inspection, one finds that stellar\n",
    "spectra contain complex features, including minima, maxima and so-called Fraunhofer lines, all caused by\n",
    "radiative processes in the outermost layers of a star. One of the basic parameters that can be derived from\n",
    "a spectrum is the effective temperature of a star which is a rough measure of the temperature of the star’s\n",
    "outermost visible layers, as well as its color.   \n",
    "\n",
    "/share/spectra contains 79200 training spectra and 7260 test spectra of stars with effective temperatures\n",
    "between 4000 Kelvin and 6000 Kelvin with a stepsize of 200 Kelvin. All spectra include various instrumental\n",
    "errors such as Gaussian noise or variable line widths, introduced by the instruments. To load the spectra\n",
    "into your notebook, use   \n",
    "\n",
    "```python\n",
    "from os import listdir\n",
    "from os.path import isfile , join\n",
    "path = \"Training\"\n",
    "spec_names = [f for f in listdir(path) if isfile(join(path ,f))]\n",
    "```\n",
    "\n",
    "which will create a list of all spectra filenames. The first four characters of the filename of a spectrum\n",
    "designate its effective temperature. To access a given *.npz file, use\n",
    "\n",
    "```python\n",
    "spec_file = str(path)+’/’+str(spec_names[0])\n",
    "spec = np.load(spec_file)\n",
    "wave = spec[\"arr_0\"][:,0]\n",
    "flux = spec[\"arr_0\"][:,1]\n",
    "```\n",
    "\n",
    "which extracts the wavelength bins (the x axis) and the flux (the y axis) of a spectrum. All spectra have 8500\n",
    "wavelength bins. Your training data array should have the shape (79200, 8500, 1), including a dummy\n",
    "dimension which can be used to divide the spectrum into channels.    \n",
    "\n",
    "For your network you can start with   \n",
    "\n",
    "```python\n",
    "keras.layers.Conv1D(12, 32, activation=’relu’, input_shape=(spec_length , 1))\n",
    "keras.layers.MaxPool1D(8)\n",
    "```\n",
    "\n",
    "followed by a flattening layer and the dense and softmaxed output layer. Train your network for two epochs.\n",
    "What’s your networks performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/share/spectra/training\"\n",
    "spec_names = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "def load_spectrum(spec_file):\n",
    "    spec = np.load(spec_file)\n",
    "    wave = spec[\"arr_0\"][:, 0] \n",
    "    flux = spec[\"arr_0\"][:, 1]\n",
    "    return wave, flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for spec_name in spec_names:\n",
    "    temp = int(spec_name[:4])\n",
    "\n",
    "    spec_file = join(path, spec_name)\n",
    "    wave, flux = load_spectrum(spec_file)\n",
    "    \n",
    "    flux = (flux - np.min(flux)) / (np.max(flux) - np.min(flux))\n",
    "    \n",
    "    X_train.append(np.expand_dims(flux, axis=-1))  \n",
    "    y_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of spectra to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train = (y_train - 4000) // 200 \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m2475/2475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.6026 - loss: 1.1661\n",
      "Epoch 2/2\n",
      "\u001b[1m2475/2475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0218\n",
      "\u001b[1m2475/2475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671us/step - accuracy: 0.9999 - loss: 0.0035\n",
      "Training accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv1D(12, 32, activation='relu', input_shape=(8500, 1)),\n",
    "    layers.MaxPool1D(8),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(11, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=2, batch_size=32)\n",
    "\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "print(f\"Training accuracy: {train_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
